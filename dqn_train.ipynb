{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magent2.environments import battle_v4\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque,defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "from dqn_model import DQN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity,observation_shape,action_shape):\n",
    "        self.capacity = capacity\n",
    "        self.observation_shape = observation_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.buffers = defaultdict(lambda: {\n",
    "            'observation': deque(maxlen=capacity),\n",
    "            'action': deque(maxlen=capacity),\n",
    "            'reward': deque(maxlen=capacity),\n",
    "            'next_observation': deque(maxlen=capacity),\n",
    "            'done': deque(maxlen=capacity),\n",
    "        })\n",
    "\n",
    "    def update_reward(self,agent_id,new_reward):\n",
    "        if agent_id not in self.buffers:\n",
    "            return\n",
    "        self.buffers[agent_id]['reward'][-1] = new_reward\n",
    "    def add(self,agent_id, observation, action, reward, next_observation, done):\n",
    "        self.buffers[agent_id]['observation'].append(observation)\n",
    "        self.buffers[agent_id]['action'].append(action)\n",
    "        self.buffers[agent_id]['reward'].append(reward)\n",
    "        self.buffers[agent_id]['next_observation'].append(next_observation)\n",
    "        self.buffers[agent_id]['done'].append(done)\n",
    "    def sample(self,batch_size):\n",
    "        if len(self.buffers.keys()) == 0 or sum(len(agent['observation']) for agent in self.buffers.values()) < batch_size:\n",
    "            return None\n",
    "        transitions = []\n",
    "        for agent_id in self.buffers.keys():\n",
    "            agent = self.buffers[agent_id]\n",
    "            for i in range(len(agent['observation'])):\n",
    "                transition = {\n",
    "                    'observation': agent['observation'][i],\n",
    "                    'action': agent['action'][i],\n",
    "                    'reward': agent['reward'][i],\n",
    "                    'next_observation': agent['next_observation'][i],\n",
    "                    'done': agent['done'][i],\n",
    "                }\n",
    "                transitions.append(transition)\n",
    "        samples_index = np.random.choice(len(transitions),batch_size,replace=False)\n",
    "        return {\n",
    "            'observation': np.array([transitions[i]['observation'] for i in samples_index]),\n",
    "            'action' :np.array([transitions[i]['action'] for i in samples_index]),\n",
    "            'reward' : np.array([transitions[i]['reward'] for i in samples_index]),\n",
    "            'next_observation' : np.array([transitions[i]['next_observation'] for i in samples_index]),\n",
    "            'done' : np.array([transitions[i]['done'] for i in samples_index]),\n",
    "        }\n",
    "    def clear(self, agent_id=None):\n",
    "        if agent_id:\n",
    "            self.buffers[agent_id]['observation'].clear()\n",
    "            self.buffers[agent_id]['action'].clear()\n",
    "            self.buffers[agent_id]['reward'].clear()\n",
    "            self.buffers[agent_id]['next_observation'].clear()\n",
    "            self.buffers[agent_id]['done'].clear()\n",
    "        else:\n",
    "            for agent_id in self.buffers:\n",
    "                self.clear(agent_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## Khởi tạo môi trường"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = battle_v4.env(map_size=45, render_mode=\"rgb_array\",max_cycles=300,attack_opponent_reward=3)\n",
    "observation_shape = env.observation_space(\"blue_0\").shape\n",
    "action_shape = env.action_space(\"blue_0\").n\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize networks\n",
    "policy_net = DQN(observation_shape, action_shape).to(device)\n",
    "red_policy = DQN(observation_shape, action_shape).to(device)\n",
    "target_net = DQN(observation_shape, action_shape).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "# try:\n",
    "#     checkpoint = torch.load(\"models/blue.pt\", map_location=device, weights_only=True)\n",
    "#     policy_net.load_state_dict(checkpoint[\"policy_net_state_dict\"])\n",
    "#     target_net.load_state_dict(checkpoint[\"target_net_state_dict\"])\n",
    "#     red_policy.load_state_dict(checkpoint[\"policy_net_state_dict\"])\n",
    "#     optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "#     episode = checkpoint[\"episode\"]\n",
    "#     print(f\"Start with episode: {episode}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"No model found!\")\n",
    "episode = 0\n",
    "\n",
    "# pretrained\n",
    "from torch_model import QNetwork\n",
    "pretrained = QNetwork(\n",
    "    env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    ")\n",
    "pretrained.load_state_dict(\n",
    "    torch.load(\"red.pt\", weights_only=True, map_location=device)\n",
    ")\n",
    "\n",
    "# trained\n",
    "from final_torch_model import QNetwork as FinalQNetwork\n",
    "trained = FinalQNetwork(\n",
    "    env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    ")\n",
    "trained.load_state_dict(\n",
    "    torch.load(\"red_final.pt\", weights_only=True, map_location=device)\n",
    ")\n",
    "\n",
    "\n",
    "buffer = ReplayBuffer(10000, observation_shape, action_shape)\n",
    "steps_done = episode\n",
    "episode_rewards = []\n",
    "episode_losses = []\n",
    "running_loss = 0.0\n",
    "num_episodes = 60\n",
    "\n",
    "def plot_durations(episode_rewards,episode_losses, show_result=False):\n",
    "    plt.figure(1)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "    losses_t = torch.tensor(episode_losses, dtype=torch.float)\n",
    "\n",
    "    plt.plot(rewards_t.numpy(), label='Reward')\n",
    "    plt.plot(losses_t.numpy(), label='Loss')\n",
    "\n",
    "    if len(rewards_t) >= 5:\n",
    "        rewards_means = rewards_t.unfold(0, 5, 1).mean(1).view(-1)\n",
    "        rewards_means = torch.cat((torch.zeros(4), rewards_means))\n",
    "        plt.plot(rewards_means.numpy(), label='Reward (mean)')\n",
    "\n",
    "    if len(losses_t) >= 5:\n",
    "        losses_means = losses_t.unfold(0, 5, 1).mean(1).view(-1)\n",
    "        losses_means = torch.cat((torch.zeros(4), losses_means))\n",
    "        plt.plot(losses_means.numpy(), label='Loss (mean)')\n",
    "    plt.legend()\n",
    "    plt.pause(0.001)\n",
    "    if not show_result:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "    else:\n",
    "        display.display(plt.gcf())\n",
    "def linear_epsilon(steps_done):\n",
    "    return max(EPS_END, EPS_START - (EPS_START - EPS_END) * (steps_done / EPS_DECAY))\n",
    "\n",
    "def policy(observation, q_network):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    if sample < linear_epsilon(steps_done):\n",
    "        return env.action_space(\"blue_0\").sample()\n",
    "    else:\n",
    "        observation = (\n",
    "            torch.Tensor(observation).to(device)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            q_values = q_network(observation)\n",
    "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(i_episode, policy_net, target_net, optimizer, episode_rewards, episode_losses, path):\n",
    "    torch.save({\n",
    "        'episode': i_episode,\n",
    "        'policy_net_state_dict': policy_net.state_dict(),\n",
    "        'target_net_state_dict': target_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_losses': episode_losses,\n",
    "    }, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    global running_loss\n",
    "\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "\n",
    "    if batch is None:\n",
    "        return\n",
    "\n",
    "    state_batch = torch.from_numpy(batch['observation']).float().to(device)\n",
    "    action_batch = torch.from_numpy(batch['action']).long().to(device)\n",
    "    reward_batch = torch.from_numpy(batch['reward']).float().to(device)\n",
    "    next_state_batch = torch.from_numpy(batch['next_observation']).float().to(device)\n",
    "    done_batch = torch.from_numpy(batch['done']).float().to(device)\n",
    "\n",
    "    # Reshape action_batch to (BATCH_SIZE, 1) for gather()\n",
    "    action_batch = action_batch.unsqueeze(1)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    non_final_mask = (done_batch == 0).squeeze()  # Create a mask for non-terminal states\n",
    "\n",
    "    # Only compute for non-terminal states\n",
    "    if non_final_mask.any():\n",
    "        next_state_values[non_final_mask] = target_net(next_state_batch[non_final_mask]).max(1).values.detach()\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with pretrained policy red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/60\n",
      "Total Reward of previous episode: -72.05\n",
      "Average Loss: 765.7060\n",
      "Epsilon: 0.89915\n",
      "----------------------------------------\n",
      "Episode 2/60\n",
      "Total Reward of previous episode: 112.45\n",
      "Average Loss: 1862.0758\n",
      "Epsilon: 0.8983\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(episode, num_episodes):\n",
    "    \n",
    "    env.reset()\n",
    "    episode_reward = 0\n",
    "    running_loss = 0.0\n",
    "    steps_done += 1\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            action = None  # Agent is dead\n",
    "            env.step(action)\n",
    "        else:\n",
    "            agent_handle = agent.split(\"_\")\n",
    "            agent_id = agent_handle[1]\n",
    "            agent_team = agent_handle[0]\n",
    "            if agent_team == \"blue\":\n",
    "\n",
    "                buffer.update_reward(agent_id, reward) # update reward of last agent's action (bad environment!)\n",
    "\n",
    "                action = policy(observation, policy_net)\n",
    "                env.step(action)\n",
    "\n",
    "                try:\n",
    "                    next_observation = env.observe(agent)\n",
    "                    agent_done = False\n",
    "                except:\n",
    "                    next_observation = None\n",
    "                    agent_done = True\n",
    "\n",
    "                reward = 0 # Wait for next time to be selected to get reward\n",
    "\n",
    "                # Store the transition in buffer\n",
    "                buffer.add(agent_id, observation, action, reward, next_observation, agent_done)\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                optimize_model()\n",
    "\n",
    "                # Soft update of the target network's weights\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            else:\n",
    "                # red agent\n",
    "                action = policy(observation, red_policy)\n",
    "                env.step(action)\n",
    "\n",
    "        # Periodically update the red agent's policy with the blue agent's learned policy\n",
    "        if i_episode % 4 == 0 and i_episode < 24:\n",
    "            # Copy all weights and biases from the blue agent's policy network to the red agent's\n",
    "            red_policy.load_state_dict(policy_net.state_dict())\n",
    "        elif i_episode == 24: # more complex (pretrained) opponent\n",
    "            red_policy.load_state_dict(pretrained.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "    # Add these lines at the end of each episode\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_losses.append(running_loss)\n",
    "\n",
    "    print(f'Episode {i_episode + 1}/{num_episodes}')\n",
    "    print(f'Total Reward of previous episode: {episode_reward:.2f}')\n",
    "    print(f'Average Loss: {running_loss:.4f}')\n",
    "    print(f'Epsilon: {linear_epsilon(steps_done)}')\n",
    "    print('-' * 40)\n",
    "    save_model(i_episode, policy_net, target_net, optimizer, episode_rewards, episode_losses, path=f\"models1/blue_{i_episode}.pt\")\n",
    "\n",
    "plot_durations(episode_rewards, episode_losses, show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with trained policy red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[224], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m buffer\u001b[38;5;241m.\u001b[39madd(agent_id, observation, action, reward, next_observation, agent_done)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[0;32m     45\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m target_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Cell \u001b[1;32mIn[222], line 4\u001b[0m, in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize_model\u001b[39m():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m running_loss\n\u001b[1;32m----> 4\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Handle cases where the buffer doesn't have enough samples yet\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[218], line 38\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(agent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[0;32m     31\u001b[0m         transition \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     32\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m: agent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m][i],\n\u001b[0;32m     33\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m: agent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m][i],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m: agent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m][i],\n\u001b[0;32m     37\u001b[0m         }\n\u001b[1;32m---> 38\u001b[0m         transitions\u001b[38;5;241m.\u001b[39mappend(transition)\n\u001b[0;32m     39\u001b[0m samples_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(transitions),batch_size,replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([transitions[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m samples_index]),\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m :np\u001b[38;5;241m.\u001b[39marray([transitions[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m samples_index]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m : np\u001b[38;5;241m.\u001b[39marray([transitions[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m samples_index]),\n\u001b[0;32m     46\u001b[0m }\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i_episode in range(episode, num_episodes):\n",
    "    \n",
    "    env.reset()\n",
    "    episode_reward = 0\n",
    "    running_loss = 0.0\n",
    "    steps_done += 1\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            action = None  # Agent is dead\n",
    "            env.step(action)\n",
    "        else:\n",
    "            agent_handle = agent.split(\"_\")\n",
    "            agent_id = agent_handle[1]\n",
    "            agent_team = agent_handle[0]\n",
    "            if agent_team == \"blue\":\n",
    "\n",
    "                buffer.update_reward(agent_id, reward)\n",
    "\n",
    "                action = policy(observation, policy_net)\n",
    "                env.step(action)\n",
    "\n",
    "                try:\n",
    "                    next_observation = env.observe(agent)\n",
    "                    agent_done = False\n",
    "                except:\n",
    "                    next_observation = None\n",
    "                    agent_done = True\n",
    "\n",
    "                reward = 0 \n",
    "\n",
    "                # Store the transition in buffer\n",
    "                buffer.add(agent_id, observation, action, reward, next_observation, agent_done)\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                optimize_model()\n",
    "\n",
    "                # Soft update of the target network's weights\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            elif agent_team == \"red\":\n",
    "                action = policy(observation, red_policy)\n",
    "                env.step(action)\n",
    "\n",
    "        if i_episode % 4 == 0 and i_episode < 24:\n",
    "            red_policy.load_state_dict(policy_net.state_dict())\n",
    "        elif i_episode == 24: \n",
    "            red_policy.load_state_dict(trained.state_dict())\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_losses.append(running_loss)\n",
    "\n",
    "    print(f'Episode {i_episode + 1}/{num_episodes}')\n",
    "    print(f'Total Reward of previous episode: {episode_reward:.2f}')\n",
    "    print(f'Average Loss: {running_loss:.4f}')\n",
    "    print(f'Epsilon: {linear_epsilon(steps_done)}')\n",
    "    print('-' * 40)\n",
    "    save_model(i_episode, policy_net, target_net, optimizer, episode_rewards, episode_losses, path=f\"models2/blue_{i_episode}.pt\")\n",
    "\n",
    "plot_durations(episode_rewards, episode_losses, show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
