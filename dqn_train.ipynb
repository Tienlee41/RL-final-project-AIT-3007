{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magent2.environments import battle_v4\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque,defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "from dqn_model import DQN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity,observation_shape,action_shape):\n",
    "        self.capacity = capacity\n",
    "        self.observation_shape = observation_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.buffers = defaultdict(lambda: {\n",
    "            'observation': deque(maxlen=capacity),\n",
    "            'action': deque(maxlen=capacity),\n",
    "            'reward': deque(maxlen=capacity),\n",
    "            'next_observation': deque(maxlen=capacity),\n",
    "            'done': deque(maxlen=capacity),\n",
    "        })\n",
    "\n",
    "    def update_reward(self,agent_id,new_reward):\n",
    "        if agent_id not in self.buffers:\n",
    "            return\n",
    "        self.buffers[agent_id]['reward'][-1] = new_reward\n",
    "        \n",
    "    def add(self,agent_id, observation, action, reward, next_observation, done):\n",
    "        self.buffers[agent_id]['observation'].append(observation)\n",
    "        self.buffers[agent_id]['action'].append(action)\n",
    "        self.buffers[agent_id]['reward'].append(reward)\n",
    "        self.buffers[agent_id]['next_observation'].append(next_observation)\n",
    "        self.buffers[agent_id]['done'].append(done)\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        if len(self.buffers.keys()) == 0 or sum(len(agent['observation']) for agent in self.buffers.values()) < batch_size:\n",
    "            return None\n",
    "        transitions = []\n",
    "        for agent_id in self.buffers.keys():\n",
    "            agent = self.buffers[agent_id]\n",
    "            for i in range(len(agent['observation'])):\n",
    "                transition = {\n",
    "                    'observation': agent['observation'][i],\n",
    "                    'action': agent['action'][i],\n",
    "                    'reward': agent['reward'][i],\n",
    "                    'next_observation': agent['next_observation'][i],\n",
    "                    'done': agent['done'][i],\n",
    "                }\n",
    "                transitions.append(transition)\n",
    "        samples_index = np.random.choice(len(transitions),batch_size,replace=False)\n",
    "        return {\n",
    "            'observation': np.array([transitions[i]['observation'] for i in samples_index]),\n",
    "            'action' :np.array([transitions[i]['action'] for i in samples_index]),\n",
    "            'reward' : np.array([transitions[i]['reward'] for i in samples_index]),\n",
    "            'next_observation' : np.array([transitions[i]['next_observation'] for i in samples_index]),\n",
    "            'done' : np.array([transitions[i]['done'] for i in samples_index]),\n",
    "        }\n",
    "    def clear(self, agent_id=None):\n",
    "        if agent_id:\n",
    "            self.buffers[agent_id]['observation'].clear()\n",
    "            self.buffers[agent_id]['action'].clear()\n",
    "            self.buffers[agent_id]['reward'].clear()\n",
    "            self.buffers[agent_id]['next_observation'].clear()\n",
    "            self.buffers[agent_id]['done'].clear()\n",
    "        else:\n",
    "            for agent_id in self.buffers:\n",
    "                self.clear(agent_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## Khởi tạo môi trường"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = battle_v4.env(map_size=45, render_mode=\"rgb_array\",max_cycles=300,attack_opponent_reward=1)\n",
    "\n",
    "observation_shape = env.observation_space(\"blue_0\").shape\n",
    "action_shape = env.action_space(\"blue_0\").n\n",
    "\n",
    "policy_net = DQN(observation_shape, action_shape).to(\"cuda\")\n",
    "\n",
    "red_policy = DQN(observation_shape, action_shape).to(\"cuda\")\n",
    "\n",
    "target_net = DQN(observation_shape, action_shape).to(\"cuda\")\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "episode = 0\n",
    "\n",
    "# pretrained model\n",
    "from torch_model import QNetwork\n",
    "pretrained = QNetwork(\n",
    "    env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    ")\n",
    "pretrained.load_state_dict(\n",
    "    torch.load(\"red.pt\", weights_only=True, map_location=\"cuda\")\n",
    ")\n",
    "\n",
    "# trained model\n",
    "from final_torch_model import QNetwork as FinalQNetwork\n",
    "trained = FinalQNetwork(\n",
    "    env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
    ")\n",
    "trained.load_state_dict(\n",
    "    torch.load(\"red_final.pt\", weights_only=True, map_location=\"cuda\")\n",
    ")\n",
    "\n",
    "\n",
    "buffer = ReplayBuffer(10000, observation_shape, action_shape)\n",
    "steps_done = episode\n",
    "episode_rewards = []\n",
    "episode_losses = []\n",
    "running_loss = 0.0\n",
    "num_episodes = 60\n",
    "\n",
    "def linear_epsilon(steps_done):\n",
    "    return max(EPS_END, EPS_START - (EPS_START - EPS_END) * (steps_done / EPS_DECAY))\n",
    "\n",
    "def policy(observation, q_network):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    if sample < linear_epsilon(steps_done):\n",
    "        return env.action_space(\"blue_0\").sample()\n",
    "    else:\n",
    "        observation = (\n",
    "            torch.Tensor(observation).to(\"cuda\")\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            q_values = q_network(observation)\n",
    "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(i_episode, policy_net, target_net, optimizer, episode_rewards, episode_losses, path):\n",
    "    torch.save({\n",
    "        'episode': i_episode,\n",
    "        'policy_net_state_dict': policy_net.state_dict(),\n",
    "        'target_net_state_dict': target_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_losses': episode_losses,\n",
    "    }, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    global running_loss\n",
    "\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    if batch is None:\n",
    "        return\n",
    "\n",
    "    state_batch = torch.from_numpy(batch['observation']).float().to(\"cuda\")\n",
    "    action_batch = torch.from_numpy(batch['action']).long().to(\"cuda\")\n",
    "    reward_batch = torch.from_numpy(batch['reward']).float().to(\"cuda\")\n",
    "    next_state_batch = torch.from_numpy(batch['next_observation']).float().to(\"cuda\")\n",
    "    done_batch = torch.from_numpy(batch['done']).float().to(\"cuda\")\n",
    "\n",
    "    action_batch = action_batch.unsqueeze(1)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=\"cuda\")\n",
    "    non_final_mask = (done_batch == 0).squeeze()\n",
    "    \n",
    "    if non_final_mask.any():\n",
    "        next_state_values[non_final_mask] = target_net(next_state_batch[non_final_mask]).max(1).values.detach()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/60\n",
      "Total Reward of previous episode: -1314.07\n",
      "Average Loss: 493.7680\n",
      "Epsilon: 0.89915\n",
      "----------------------------------------\n",
      "Episode 2/60\n",
      "Total Reward of previous episode: -1476.27\n",
      "Average Loss: 1326.9272\n",
      "Epsilon: 0.8983\n",
      "----------------------------------------\n",
      "Episode 3/60\n",
      "Total Reward of previous episode: -1431.66\n",
      "Average Loss: 965.0810\n",
      "Epsilon: 0.89745\n",
      "----------------------------------------\n",
      "Episode 4/60\n",
      "Total Reward of previous episode: -1397.04\n",
      "Average Loss: 746.9168\n",
      "Epsilon: 0.8966000000000001\n",
      "----------------------------------------\n",
      "Episode 5/60\n",
      "Total Reward of previous episode: -1353.14\n",
      "Average Loss: 841.8537\n",
      "Epsilon: 0.89575\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m buffer\u001b[38;5;241m.\u001b[39madd(agent_id, observation, action, reward, next_observation, agent_done)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[0;32m     44\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m target_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Cell \u001b[1;32mIn[10], line 34\u001b[0m, in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Optimize the model\u001b[39;00m\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# In-place gradient clipping\u001b[39;00m\n\u001b[0;32m     36\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_value_(policy_net\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\NAME\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\NAME\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\NAME\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for i_episode in range(episode, num_episodes):\n",
    "    env.reset()\n",
    "    episode_reward = 0\n",
    "    running_loss = 0.0\n",
    "    steps_done += 1\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        done = termination or truncation\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            action = None\n",
    "            env.step(action)\n",
    "        else:\n",
    "            agent_handle = agent.split(\"_\")\n",
    "            agent_id = agent_handle[1]\n",
    "            agent_team = agent_handle[0]\n",
    "            if agent_team == \"blue\":\n",
    "\n",
    "                buffer.update_reward(agent_id, reward)\n",
    "\n",
    "                action = policy(observation, policy_net)\n",
    "                env.step(action)\n",
    "\n",
    "                try:\n",
    "                    next_observation = env.observe(agent)\n",
    "                    agent_done = False\n",
    "                except:\n",
    "                    next_observation = None\n",
    "                    agent_done = True\n",
    "                reward = 0 \n",
    "                buffer.add(agent_id, observation, action, reward, next_observation, agent_done)\n",
    "\n",
    "                optimize_model()\n",
    "\n",
    "                # Soft update of the target network's weights\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            elif agent_team == \"red\":\n",
    "                action = env.action_space(agent).sample()\n",
    "                env.step(action)\n",
    "\n",
    "        if (i_episode + 1) % 3 == 0:\n",
    "            red_policy.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_losses.append(running_loss)\n",
    "\n",
    "    print(f'Episode {i_episode + 1}/{num_episodes}')\n",
    "    print(f'Total Reward of previous episode: {episode_reward:.2f}')\n",
    "    print(f'Average Loss: {running_loss:.4f}')\n",
    "    print(f'Epsilon: {linear_epsilon(steps_done)}')\n",
    "    print('-' * 40)\n",
    "    save_model(i_episode, policy_net, target_net, optimizer, episode_rewards, episode_losses, path=f\"models/blue_{i_episode}.pt\")\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
